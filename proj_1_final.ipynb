{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\dlee2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\dlee2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\dlee2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\dlee2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD, PCA \n",
    "from sklearn.multioutput import MultiOutputClassifier \n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.kernel_approximation import Nystroem\n",
    "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, roc_auc_score, f1_score\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from gensim.models import Word2Vec\n",
    "import nltk\n",
    "for package in ['stopwords','punkt','wordnet', 'omw-1.4']:\n",
    "    nltk.download(package) \n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "import string\n",
    "import xgboost as xgb"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project 1 - NLP and Text Classification\n",
    "\n",
    "For this project you will need to classify some angry comments into their respective category of angry. The process that you'll need to follow is (roughly):\n",
    "<ol>\n",
    "<li> Use NLP techniques to process the training data. \n",
    "<li> Train model(s) to predict which class(es) each comment is in.\n",
    "    <ul>\n",
    "    <li> A comment can belong to any number of classes, including none. \n",
    "    </ul>\n",
    "<li> Generate predictions for each of the comments in the test data. \n",
    "<li> Write your test data predicitions to a CSV file, which will be scored. \n",
    "</ol>\n",
    "\n",
    "You can use any models and NLP libraries you'd like. Think aobut the problem, look back to see if there's anything that might help, give it a try, and see if that helps. We've regularly said we have a \"toolkit\" of things that we can use, we generally don't know which ones we'll need, but here you have a pretty simple goal - if it makes it more accurate, it helps. There's not one specific solution here, there are lots of things that you could do. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Data\n",
    "\n",
    "Use the training data to train your prediction model(s). Each of the classification output columns (toxic to the end) is a human label for the comment_text, assessing if it falls into that category of \"rude\". A comment may fall into any number of categories, or none at all. Membership in one output category is <b>independent</b> of membership in any of the other classes (think about this when you plan on how to make these predictions - it may also make it easier to split work amongst a team...). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"train.csv.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 159571 entries, 0 to 159570\n",
      "Data columns (total 8 columns):\n",
      " #   Column         Non-Null Count   Dtype \n",
      "---  ------         --------------   ----- \n",
      " 0   id             159571 non-null  object\n",
      " 1   comment_text   159571 non-null  object\n",
      " 2   toxic          159571 non-null  int64 \n",
      " 3   severe_toxic   159571 non-null  int64 \n",
      " 4   obscene        159571 non-null  int64 \n",
      " 5   threat         159571 non-null  int64 \n",
      " 6   insult         159571 non-null  int64 \n",
      " 7   identity_hate  159571 non-null  int64 \n",
      "dtypes: int64(6), object(2)\n",
      "memory usage: 9.7+ MB\n"
     ]
    }
   ],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no nulls."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View Dataset Balance ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Target column names\n",
    "target_cols = train_df.iloc[:,2:].columns.to_list()\n",
    "target_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    144277\n",
      "1     15294\n",
      "Name: toxic, dtype: int64\n",
      "Accuracy if all classifed as '0': 0.9041555169799024\n",
      "\n",
      "0    157976\n",
      "1      1595\n",
      "Name: severe_toxic, dtype: int64\n",
      "Accuracy if all classifed as '0': 0.9900044494300343\n",
      "\n",
      "0    151122\n",
      "1      8449\n",
      "Name: obscene, dtype: int64\n",
      "Accuracy if all classifed as '0': 0.947051782592075\n",
      "\n",
      "0    159093\n",
      "1       478\n",
      "Name: threat, dtype: int64\n",
      "Accuracy if all classifed as '0': 0.9970044682304429\n",
      "\n",
      "0    151694\n",
      "1      7877\n",
      "Name: insult, dtype: int64\n",
      "Accuracy if all classifed as '0': 0.9506363938309592\n",
      "\n",
      "0    158166\n",
      "1      1405\n",
      "Name: identity_hate, dtype: int64\n",
      "Accuracy if all classifed as '0': 0.9911951419744189\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# View dataset balance and establish baseline accuracy\n",
    "for column in target_cols:\n",
    "    print(train_df[column].value_counts())\n",
    "    print(\"Accuracy if all classifed as '0':\", train_df[column].value_counts().loc[0] / (train_df[column].value_counts().loc[0] + train_df[column].value_counts().loc[1]))\n",
    "    print()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Data for Model Tuning and Creation ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All 0: 143346\n",
      "Some 1: 16225\n"
     ]
    }
   ],
   "source": [
    "train_df_0 = train_df[train_df.iloc[:, 2:].sum(axis=1) == 0] # rows that have all 0 in target columns\n",
    "train_df_1 = train_df[train_df.iloc[:, 2:].sum(axis=1) != 0] # rows that have at least one 1 in one of the target columns\n",
    "print('All 0:', len(train_df_0))\n",
    "print('Some 1:', len(train_df_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>21291</th>\n",
       "      <td>382200f22191e876</td>\n",
       "      <td>unblock me you prick \\n\\nyeah</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41122</th>\n",
       "      <td>6dbf3883f3f98172</td>\n",
       "      <td>10fags fags fcockc dick0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51079</th>\n",
       "      <td>889fd1326b132490</td>\n",
       "      <td>What the hell are you on about? 81.144.199.2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68339</th>\n",
       "      <td>b6cf05d648630e9b</td>\n",
       "      <td>Well, if you still think that way , why dont y...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84737</th>\n",
       "      <td>e2bcde12ffef52e9</td>\n",
       "      <td>, you certainly are knowledgable on the topic....</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     id                                       comment_text  \\\n",
       "21291  382200f22191e876                      unblock me you prick \\n\\nyeah   \n",
       "41122  6dbf3883f3f98172                           10fags fags fcockc dick0   \n",
       "51079  889fd1326b132490       What the hell are you on about? 81.144.199.2   \n",
       "68339  b6cf05d648630e9b  Well, if you still think that way , why dont y...   \n",
       "84737  e2bcde12ffef52e9  , you certainly are knowledgable on the topic....   \n",
       "\n",
       "       toxic  severe_toxic  obscene  threat  insult  identity_hate  \n",
       "21291      1             0        0       0       0              0  \n",
       "41122      1             0        1       0       0              0  \n",
       "51079      1             0        0       0       0              0  \n",
       "68339      1             0        0       0       0              0  \n",
       "84737      0             0        1       0       0              0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View some angry comments\n",
    "train_df_1.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a \"more\" balanced sample so we can account for more 'angry' comments when training and tuning\n",
    "# train_df_0 = train_df_0.sample(30000) \n",
    "# train_df_1 = train_df_1.sample(15000)\n",
    "# train_df_ = pd.concat([train_df_1, train_df_0], axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_ = train_df # to see train test split results on all of training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    144277\n",
      "1     15294\n",
      "Name: toxic, dtype: int64\n",
      "Accuracy if all classifed as '0': 0.9041555169799024\n",
      "\n",
      "0    157976\n",
      "1      1595\n",
      "Name: severe_toxic, dtype: int64\n",
      "Accuracy if all classifed as '0': 0.9900044494300343\n",
      "\n",
      "0    151122\n",
      "1      8449\n",
      "Name: obscene, dtype: int64\n",
      "Accuracy if all classifed as '0': 0.947051782592075\n",
      "\n",
      "0    159093\n",
      "1       478\n",
      "Name: threat, dtype: int64\n",
      "Accuracy if all classifed as '0': 0.9970044682304429\n",
      "\n",
      "0    151694\n",
      "1      7877\n",
      "Name: insult, dtype: int64\n",
      "Accuracy if all classifed as '0': 0.9506363938309592\n",
      "\n",
      "0    158166\n",
      "1      1405\n",
      "Name: identity_hate, dtype: int64\n",
      "Accuracy if all classifed as '0': 0.9911951419744189\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# View target balance of our sample\n",
    "for column in target_cols:\n",
    "    print(train_df_[column].value_counts())\n",
    "    print(\"Accuracy if all classifed as '0':\", train_df_[column].value_counts().loc[0] / (train_df_[column].value_counts().loc[0] + train_df_[column].value_counts().loc[1]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_ = train_df_['comment_text']\n",
    "y_ = train_df_[target_cols] # Y contains multiple columns for our MultiOutputClassifier"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process the Training Data ###"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining a Preprocessing Function:\n",
    "\n",
    "<ul>\n",
    "<li> The preprocess function is designed to preprocess individual text documents by removing unnecessary words, characters, and noise that may not be useful for downstream analysis.\n",
    "<li> The preprocessor will be applied to the training data and the test data so it is ready for the next steps in the model.\n",
    "<li> It takes into account things like punctuation and different formats of words so our testing data is more likely to the recognized by the our model.\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english')) # list of stop words \n",
    "\n",
    "# Separate text into list of lemmatized tokens\n",
    "class preprocess(object):\n",
    "    def __init__(self, stop_words):\n",
    "        self.stop_words = stop_words\n",
    "        from nltk.stem import WordNetLemmatizer\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "    def __call__(self, doc):\n",
    "        tokens = word_tokenize(doc)\n",
    "        filtered_tok = []\n",
    "        for tok in tokens:\n",
    "            if tok not in stop_words:\n",
    "                tok = re.sub('\\W+','', tok) # Punctuation strip\n",
    "                tmp = self.lemmatizer.lemmatize(tok)\n",
    "                if len(tmp) >= 1: \n",
    "                    filtered_tok.append(tok)\n",
    "        return \" \".join(filtered_tok)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying Preprocessor function to Our Training Dataset:\n",
    "<ul>\n",
    "<li> Each comment of our training dataset will be cleaned with out preprocessor and stored in a variable 'x_clean'\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess text with stop words from NLTK\n",
    "clean = preprocess(stop_words)\n",
    "x_clean = [clean(x) for x in x_]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train-Test Split\n",
    "<ul>\n",
    "<li> Split the data into training and testing sets so we can evaluate our accuracy on populated target columns\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x_clean, y_, test_size=0.2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec ###\n",
    "\n",
    "<ul>\n",
    "<li> We ran multiple trials and models with a Word2Vec model assuming that it's more advanced algorithms could capture relationships between 'trigger' words that would help classify angry texts.\n",
    "<li> Most of the models did a poor job at classifying angry texts, usually labelling everything as 0. \n",
    "<li> Ultimately, we had better results from TF-IDF vectorization. \n",
    "<li> This section is commented out but includes some of the trials we did with Word2Vec.\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train test split \n",
    "# x = train_df_['comment_text']\n",
    "# y = train_df_[target_cols]\n",
    "\n",
    "# x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Tokenize train and test split of training data\n",
    "# x_train_tok = [tok(x) for x in x_train]\n",
    "# x_test_tok = [tok(x) for x in x_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign list of tokens an average value for each vector\n",
    "# class MeanEmbeddingVectorizer(object):\n",
    "#     def __init__(self, word2vec):\n",
    "#         self.word2vec = word2vec\n",
    "#         # if a text is empty we should return a vector of zeros\n",
    "#         # with the same dimensionality as all the other vectors\n",
    "#         self.dim = len(next(iter(word2vec.values())))\n",
    "\n",
    "#     def fit(self, X, y):\n",
    "#         return self\n",
    "\n",
    "#     def transform(self, X):\n",
    "#         return np.array([\n",
    "#             np.mean([self.word2vec[w] for w in words if w in self.word2vec]\n",
    "#                     or [np.zeros(self.dim)], axis=0)\n",
    "#             for words in X\n",
    "#         ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# w2v_model = Word2Vec(x_train_tok, min_count=1, vector_size=300) # Train Word2Vec model with tokenized train split, words evaluated in 200 dimensions\n",
    "# w2v = dict(zip(w2v_model.wv.index_to_key, w2v_model.wv.vectors)) # 'scores' for each word in 200 dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Give mean score for comment in each of the 200 dimensions\n",
    "# # train split and test split need to be in the same format to be plugged into model\n",
    "# model_ = MeanEmbeddingVectorizer(w2v) \n",
    "# x_train_vectors_w2v = model_.transform(x_train_tok) \n",
    "# x_val_vectors_w2v = model_.transform(x_test_tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # LinearSVC with tuning to find hyperparameters that improve accuracy\n",
    "# clf2 = LinearSVC(dual=False, C=2)\n",
    "\n",
    "# pipe_steps2 = [\n",
    "#     (\"scale\", MinMaxScaler()), \n",
    "#     (\"kernel_approx\", Nystroem(n_components=100, kernel='linear')),\n",
    "#     (\"model\", MultiOutputClassifier(clf2))\n",
    "#     ]\n",
    "# pipe_2 = Pipeline(steps=pipe_steps2)\n",
    "\n",
    "# # params = {\n",
    "# # #     # \"kernel_approx__kernel\": ['linear', 'poly', 'rbf'], # Linear was best\n",
    "# #     # \"model__estimator__C\": [1, 2, 3, 4, 5], # 2 was best\n",
    "# # #     # \"model__estimator__class_weight\": [{0: 0.5, 1: 3}, {0: 0.5, 1: 2}, {0: 1, 1: 2}, {0: 1, 1: 1}], # weights C differently for each class to adjust for imbalance, 1 and 1 was best\n",
    "# #     }\n",
    "# # grid_model = GridSearchCV(estimator=pipe_2, param_grid=params, n_jobs=-1) \n",
    "\n",
    "# # grid_model.fit(x_train_vectors_w2v, y_train)\n",
    "# # best = grid_model.best_estimator_\n",
    "# best = pipe_2\n",
    "# best.fit(x_train_vectors_w2v, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preds2 = best.predict(x_val_vectors_w2v)\n",
    "# pred2_df = pd.DataFrame(preds2, columns=target_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for column in pred2_df.columns:\n",
    "#     print(column)\n",
    "#     print('Accuracy:', accuracy_score(y_test[column], pred2_df[column]))\n",
    "#     print(classification_report(y_test[column], pred2_df[column], zero_division=1))\n",
    "#     sns.heatmap(confusion_matrix(y_test[column], pred2_df[column]), annot=True)\n",
    "#     plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Model ###"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Out of the classifiers we tested, LinearSVC had quick run times while being the most accurate.\n",
    "\n",
    "Our model is a pipeline with the following steps:\n",
    "<ul>\n",
    "<li> TF-IDF Vectorizer to convert cleaned text into dataframe of vectors, keeping 15000 features, converting all tokens to lowercase for consistency\n",
    "<li> LinearSVC with C=1.5 obtained through GridSearchCV\n",
    "<li> LinearSVC is placed in MultiOutputClassifier to fit a classifier and make predictions on each target column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('tfidf',\n",
       "                 TfidfVectorizer(max_features=15000, strip_accents='unicode')),\n",
       "                ('model', MultiOutputClassifier(estimator=LinearSVC(C=1.5)))])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use TfidfVectorizer to convert text into numerical features\n",
    "tfidf = TfidfVectorizer(max_features=15000, strip_accents=\"unicode\", lowercase=True) # 15000 max_features from gridsearch\n",
    "\n",
    "# # Tried different classifiers\n",
    "# clf = SVC()\n",
    "clf = LinearSVC(C=1.5) # C=1.5 from gridsearch\n",
    "# clf = LogisticRegression()\n",
    "# clf = RandomForestClassifier()\n",
    "# clf = SGDClassifier()\n",
    "# clf = xgb.XGBClassifier() # Tried booster: 'gblinear', 'gbtree'\n",
    "\n",
    "pipe_steps = [\n",
    "    (\"tfidf\", tfidf),\n",
    "    (\"model\", MultiOutputClassifier(clf)) # MultiOutputClassifer fits an estimator for each classification problem\n",
    "    ]\n",
    "pipe_ = Pipeline(steps=pipe_steps)\n",
    "\n",
    "pipe_.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format predictions in a dataframe\n",
    "preds = pipe_.predict(x_test)\n",
    "pred_df = pd.DataFrame(preds,columns=target_cols)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate model on multiple metrics for each column:\n",
    "<ul>\n",
    "<li> AUC Score\n",
    "<li> Accuracy\n",
    "<li> Classification Report\n",
    "<li> Confusion Matrix\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toxic\n",
      "AUC Score: 0.8385977075678189\n",
      "Accuracy: 0.9589534701550995\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98     28903\n",
      "           1       0.85      0.69      0.76      3012\n",
      "\n",
      "    accuracy                           0.96     31915\n",
      "   macro avg       0.91      0.84      0.87     31915\n",
      "weighted avg       0.96      0.96      0.96     31915\n",
      "\n",
      "\n",
      "severe_toxic\n",
      "AUC Score: 0.6528347379096076\n",
      "Accuracy: 0.9911953626821244\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      1.00     31613\n",
      "           1       0.56      0.31      0.40       302\n",
      "\n",
      "    accuracy                           0.99     31915\n",
      "   macro avg       0.78      0.65      0.70     31915\n",
      "weighted avg       0.99      0.99      0.99     31915\n",
      "\n",
      "\n",
      "obscene\n",
      "AUC Score: 0.8432492548806547\n",
      "Accuracy: 0.978850070499765\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99     30241\n",
      "           1       0.88      0.69      0.77      1674\n",
      "\n",
      "    accuracy                           0.98     31915\n",
      "   macro avg       0.93      0.84      0.88     31915\n",
      "weighted avg       0.98      0.98      0.98     31915\n",
      "\n",
      "\n",
      "threat\n",
      "AUC Score: 0.637201586304382\n",
      "Accuracy: 0.9975873413755287\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     31835\n",
      "           1       0.54      0.28      0.36        80\n",
      "\n",
      "    accuracy                           1.00     31915\n",
      "   macro avg       0.77      0.64      0.68     31915\n",
      "weighted avg       1.00      1.00      1.00     31915\n",
      "\n",
      "\n",
      "insult\n",
      "AUC Score: 0.7813606317816376\n",
      "Accuracy: 0.9708600971330096\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98     30319\n",
      "           1       0.79      0.57      0.66      1596\n",
      "\n",
      "    accuracy                           0.97     31915\n",
      "   macro avg       0.88      0.78      0.82     31915\n",
      "weighted avg       0.97      0.97      0.97     31915\n",
      "\n",
      "\n",
      "identity_hate\n",
      "AUC Score: 0.6467550175177366\n",
      "Accuracy: 0.9924173586088046\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      1.00     31637\n",
      "           1       0.64      0.29      0.40       278\n",
      "\n",
      "    accuracy                           0.99     31915\n",
      "   macro avg       0.82      0.65      0.70     31915\n",
      "weighted avg       0.99      0.99      0.99     31915\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate our model\n",
    "for column in pred_df.columns:\n",
    "    print(column)\n",
    "    print('AUC Score:', roc_auc_score(y_test[column], pred_df[column]))\n",
    "    print('Accuracy:', accuracy_score(y_test[column], pred_df[column]))\n",
    "    print(classification_report(y_test[column], pred_df[column], zero_division=1))\n",
    "    # sns.heatmap(confusion_matrix(y_test[column], pred_df[column]), annot=True)\n",
    "    plt.show()\n",
    "    print()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Tuning ###"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our Gridsearch trials revealed:\n",
    "<ul>\n",
    "<li> A more optimal C of 1.5\n",
    "<li> Our model was better off without any kernel approximation before our LinearSVC. \n",
    "<li> A TF-IDF max features of around 15000 scored best\n",
    "</ul>\n",
    "\n",
    "Our GridSearch and results are commented out for faster run times. The hyperparameters have been plugged into the model above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline_steps = [\n",
    "#     (\"tfidf\", tfidf),\n",
    "#     # (\"kernel_approx\", Nystroem(kernel='linear')), # Better with no kernel_approximation\n",
    "#     (\"classifier\", MultiOutputClassifier(clf))\n",
    "# ]\n",
    "\n",
    "# pipeline = Pipeline(steps=pipeline_steps)\n",
    "\n",
    "# params = {\n",
    "#     \"classifier__estimator__C\": [0.5, 1, 1.5, 2], # 1.5 yielded the best results\n",
    "#     \"tfidf__max_features\": [5000, 8000, 10000, 15000, 20000]\n",
    "#     # \"kernel_approx__kernel\": ['poly', 'linear', 'rbf']\n",
    "# }\n",
    "\n",
    "# gs_model = GridSearchCV(estimator=pipeline, param_grid=params)\n",
    "# gs_model.fit(x_train, y_train)\n",
    "\n",
    "# best = gs_model.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions = best.predict(x_test)\n",
    "# df_predictions = pd.DataFrame(predictions, columns=target_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Evaluate our model\n",
    "# for column in target_cols:\n",
    "#     print(column)\n",
    "#     print('AUC Score:', roc_auc_score(y_test[column], df_predictions[column]))\n",
    "#     print('Accuracy:', accuracy_score(y_test[column], df_predictions[column]))\n",
    "#     print(classification_report(y_test[column], df_predictions[column], zero_division=1))\n",
    "#     # sns.heatmap(confusion_matrix(y_test[column], df_predictions[column]), annot=True)\n",
    "#     plt.show()\n",
    "#     print()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Training ###\n",
    "\n",
    "Train our model on all of the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('tfidf',\n",
       "                 TfidfVectorizer(max_features=15000, strip_accents='unicode')),\n",
       "                ('model', MultiOutputClassifier(estimator=LinearSVC(C=1.5)))])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Final model\n",
    "best = pipe_\n",
    "\n",
    "# Train on all of the data\n",
    "# x_clean is all of the preprocessed train_df comments \n",
    "best.fit(x_clean, y_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Yo bitch Ja Rule is more succesful then you'll...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>== From RfC == \\n\\n The title is fine as it is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>\" \\n\\n == Sources == \\n\\n * Zawe Ashton on Lap...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>:If you have a look back at the source, the in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>I don't anonymously edit articles at all.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                       comment_text\n",
       "0   1  Yo bitch Ja Rule is more succesful then you'll...\n",
       "1   2  == From RfC == \\n\\n The title is fine as it is...\n",
       "2   3  \" \\n\\n == Sources == \\n\\n * Zawe Ashton on Lap...\n",
       "3   4  :If you have a look back at the source, the in...\n",
       "4   5          I don't anonymously edit articles at all."
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.read_csv(\"test.csv\")\n",
    "test_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing our test data increases its chances of its tokenized text to be meaningful to our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "test_clean = [clean(x) for x in test_df['comment_text']]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Classifications on the Test Data ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "test_preds = best.predict(test_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153159</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153160</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153161</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153162</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153163</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>153164 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        toxic  severe_toxic  obscene  threat  insult  identity_hate\n",
       "0           1             0        1       0       1              0\n",
       "1           0             0        0       0       0              0\n",
       "2           0             0        0       0       0              0\n",
       "3           0             0        0       0       0              0\n",
       "4           0             0        0       0       0              0\n",
       "...       ...           ...      ...     ...     ...            ...\n",
       "153159      0             0        0       0       0              0\n",
       "153160      0             0        0       0       0              0\n",
       "153161      0             0        0       0       0              0\n",
       "153162      0             0        0       0       0              0\n",
       "153163      1             0        1       0       1              0\n",
       "\n",
       "[153164 rows x 6 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a dataframe of our classifications\n",
    "test_preds_df = pd.DataFrame(test_preds, columns=target_cols)\n",
    "test_preds_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output Details, Submission Info, and Example Submission\n",
    "\n",
    "For this project, please output your predictions in a CSV file. The structure of the CSV file should match the structure of the example below. \n",
    "\n",
    "The output should contain one row for each row of test data, complete with the columns for ID and each classification.\n",
    "\n",
    "Into Moodle please submit:\n",
    "<ul>\n",
    "<li> Your notebook file(s). I'm not going to run them, just look. \n",
    "<li> Your sample submission CSV. This will be evaluated for accuracy against the real labels; only a subset of the predictions will be scored. \n",
    "</ul>\n",
    "\n",
    "It is REALLY, REALLY, REALLY important the the structure of your output matches the specifications. The accuracies will be calculated by a script, and it is expecting a specific format. \n",
    "\n",
    "### Sample Evaluator\n",
    "\n",
    "The file prediction_evaluator.ipynb contains an example scoring function, scoreChecker. This function takes a sumbission and an answer key, loops through, and evaluates the accuracy. You can use this to verify the format of your submission. I'm going to use the same function to evaluate the accuracy of your submission, against the answer key (unless I made some mistake in this counting function)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  toxic  severe_toxic  obscene  threat  insult  identity_hate\n",
       "0   1      1             0        1       0       1              0\n",
       "1   2      0             0        0       0       0              0\n",
       "2   3      0             0        0       0       0              0\n",
       "3   4      0             0        0       0       0              0\n",
       "4   5      0             0        0       0       0              0"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_df = pd.concat([test_df['id'], test_preds_df], axis=1)\n",
    "out_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write DF to CSV. Please keep the \"out.csv\" filename. Moodle will auto-preface it with an identifier when I download it. \n",
    "# This command should work with your dataframe of predictions. \n",
    "out_df.to_csv('out.csv', index=False)  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating Our Classifications on the Test Data ###"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View a single comment and its classifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\" \\n : \\n :*\"\"I hope you die of something you ate\"\"  \\n :*\"\"She got shot with the whore makeup gun?\"\"  \\n :*\"\"Vani you ignorant slut\"\"  \\n :*\"\"get some real dick in your boney ass diet\"\"  \\n :*\"\"when I called you a \\'dumb cunt\\' I did not mean to imply you were a \\'dumb woman\\' [...] I think you are a dumb human being\"\" \\n :*\"\"you are a stupid female. kill yourself.\"\" \\n :*\"\"This is just for you Food Babe you\\'re an ugly twat\"\" (Image captioned \"\"What organ stays warm inside of a dead girl\\'s body? My Dick\"\") \\n :Top quality criticism from the scientific community m8...   \"'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.iloc[141859]['comment_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id               141860\n",
       "toxic                 1\n",
       "severe_toxic          1\n",
       "obscene               1\n",
       "threat                0\n",
       "insult                1\n",
       "identity_hate         0\n",
       "Name: 141859, dtype: int64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_df.iloc[141859]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View some of the comments we classified as threats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "threats = out_df[out_df['threat'] == 1].index.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56        I WILL BURN YOU TO HELL IF YOU REVOKE MY TALK ...\n",
       "277       T IS PEOPLE LIKE YOU THAT MAKE WIKIPEDIA HORRI...\n",
       "391                   and im gonna rape your dog in the ass\n",
       "1053                         WILL HUNT YOU DOWN SUCKA!!!!!!\n",
       "1071      IT IS USERS LIKE YOU THAT MAKE WIKIPEDIA A SHI...\n",
       "                                ...                        \n",
       "150611    == YA YA YA YA == \\n\\n YA YA YA YA! KILL KILL ...\n",
       "151522    . \\n\\n I AM A HABSBURG AND I PROMISE I'M GOING...\n",
       "152327    I will meet you one day and stab you to death ...\n",
       "152343    All I ever wanted to do was kill some humans! ...\n",
       "152675                  Baylor kids like to kill themselves\n",
       "Name: comment_text, Length: 423, dtype: object"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.iloc[threats]['comment_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grading\n",
    "\n",
    "The grading for this is split between accuracy and well written code:\n",
    "<ul>\n",
    "<li> 75% - Accuracy. The most accurate will get 100% on this, the others will be scaled down from there. \n",
    "<li> 25% - Code quality. Can the code be followed and made sense of - i.e. comments, sections, titles. \n",
    "</ul>"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4d722d3adfa415172c1f5238b519fb86b488acdae450fd691ab06c09f4ca9173"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('ml3950': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
